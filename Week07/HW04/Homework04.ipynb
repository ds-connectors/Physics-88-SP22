{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete this homework assignment in code cells in the iPython notebook. Include comments in your code when necessary.  Please rename the notebook as SIS ID_HW04.ipynb (your student ID number) and save the notebook once you have executed it as a PDF  (note, that when saving as PDF you don't want to use the option with latex because it crashes, but rather the one to save it directly as a PDF). \n",
    "\n",
    "For questions that ask you to make an interpretation, please write a sentence or two explaining your response. You can use \"Markdown\" language to create a cell (like this one) which is ordinary text instead of using code. To do this, create a new cell, and then look at the bar above which has a picture of a floppy disk. On the right side is a drop down menu that allows you change whether a cell is \"Markdown\" or \"Code\".\n",
    "\n",
    "**The homework should be submitted on bCourses under the Assignments tab (both the .ipynb and .pdf files). Please label it by your student ID number (SIS ID)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will verify the Central Limit Theorem and reproduced a plot I showed in class (https://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:Dice_sum_central_limit_theorem.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function that returns $n$ integer random numbers, uniformly disributed between 1 and 6, inclusively. This represents $n$ throws of a fair 6-sided die. The value that comes up at each throw will be called the \"score\".\n",
    "1. Generate a distribution of 1000 dice throws and plot it as a  histogram normalized to unit area. Compute the mean $\\mu_1$ and standard deviation $\\sigma_1$ of this distribution. Compare your numerical result to the analytical calculation. \n",
    "1. Generate 1000 sets of throws of $N=2,3,4,5,10$ dice, computing the total sum of dice scores for each set. For each value of $N$, plot the distribution of total scores, and compute the mean $\\mu_N$ and standard deviation $\\sigma_N$ of each distribution. This should be similar to the plot at the link above.\n",
    "1. Plot the standard deviation $\\sigma_N$ as a function of $N$. Does it follow the Central Limit Theorem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Problem 2: Parity-violating asymmetry\n",
    "\n",
    "The data sample for this problem comes from the <a href=\"http://www.slac.stanford.edu/exp/e158\">E158</a> experiment at SLAC (a national lab near that Junior university across the Bay). E158 measured a parity-violating asymmetry in Møller (electron-electron) scattering. This was a fixed-target experiment, which scattered longitudinally-polarized electrons off atomic (unpolarized) electrons in the 1.5m liquid hydrogen target. The data below contains a snapshot of 10,000 \"events\" from this experiment (overall, the experiment collected almost 400 million such events over the course of about 4 months). Each event actually records a pair of pulses: one for the right-handed electron (spin pointing along momentum) and one for the left-handed electron. For each event, we record 4 variables:\n",
    "\n",
    "* Counter: event index\n",
    "* Asymmetry: \"raw\" cross section asymmetry $A_{raw}$ from one of the detector channels (there are 50 of these overall). The cross section asymmetry is defined as \n",
    "$A_{raw} = \\frac{\\sigma_R-\\sigma_L}{\\sigma_R+\\sigma_L}$\n",
    "The asymmetry is recorded in units of PPM (parts per million). It is called \"raw\" because corrections due to the difference in beam properties at the target are not yet applied.\n",
    "* DeltaX: difference in beam position $\\Delta X=X_R-X_L$ at the target in X direction in microns (with the convention that the beam is traveling along Z)\n",
    "* DeltaY: difference in beam position $\\Delta Y=Y_R-Y_L$ at the target in Y direction in microns\n",
    "\n",
    "The data sample is provided in plain text format as the file <tt>asymdata.txt</tt>. Questions for this analysis:\n",
    "\n",
    "1. Read the data from the file, and plot distributions of $A_{raw}$, $\\Delta X$, and $\\Delta Y$. \n",
    "1. Compute the mean of the raw asymmetry distribution and its statistical uncertainty.\n",
    "1. Plot $A_{raw}$ vs $\\Delta X$, $A_{raw}$ vs $\\Delta Y$, and $\\Delta X$ vs $\\Delta Y$ as scatter plots. \n",
    "1. Compute the correlation coefficients <tt>Corr(Asym,DeltaX)</tt>, <tt>Corr(Asym,DeltaY)</tt>, and <tt>Corr(DeltaX,DeltaY)</tt>. See lecture notes, Workshop05.ipynb and Workshop05_optional.ipynb or https://en.wikipedia.org/wiki/Pearson_correlation_coefficient for additional help understanding correlation coefficients. Which variables are approximately independent of each other ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Gamma-ray peak\n",
    "\n",
    "[Some of you may recognize this problem from Advanced Lab's Error Analysis Exercise. That's not an accident. You may also recognize this dataset from Homework04. That's not an accident either.]\n",
    "\n",
    "You are given a dataset (`peak.dat`) from a gamma-ray experiment consisting of ~1000 hits. Each line in the file corresponds to one recorded gamma-ray event, and stores the the measured energy of the gamma-ray (in MeV). We will assume that the energies are randomly distributed about a common mean, and that each event is uncorrelated to others. Read the dataset from the enclosed file and:\n",
    "1. Produce a histogram of the distribution of energies. Choose the number of bins wisely, i.e. so that the width of each bin is smaller than the width of the peak, and at the same time so that the number of entries in the most populated bin is relatively large. Since this plot represents randomly-collected data, plotting error bars would be appropriate.\n",
    "1. Compute the mean and standard deviation of the distribution of energies and their statistical uncertainties. Assume the distribution is Gaussian and see the lecture notes for the formulas for the mean and variance of the sample and the formulas for the errors on these quantities. \n",
    "1. Compute the fraction of events contained within $\\pm 1\\sigma$ of the mean, $\\pm 2\\sigma$ of the mean, and $\\pm 3\\sigma$ of the mean (where $\\sigma$ is the standard deviation you computed in Part 2). Compare these fractions with the quantiles of the Gaussian distribution (see lecture notes) ? \n",
    "1. Fit the distribution to a Gaussian function using an unbinned fit (<i>Hint:</i> use <tt>scipi.stats.norm.fit()</tt> function), and compare the parameters of the fitted Gaussian with the mean and standard deviation computed in Part 2\n",
    "1. Fit the distribution to a Gaussian function using a binned least-squares fit (<i>Hint:</i> use <tt>scipy.optimize.curve_fit()</tt> function), and compare the parameters of the fitted Gaussian and their uncertainties to the parameters obtained in the unbinned fit above. \n",
    "1. Re-make your histogram from (1) with twice as many bins, and repeat the binned least-squares fit from (3) on the new histogram. How sensitive are your results to binning ? \n",
    "1. How consistent is the distribution with a Gaussian? In other words, compare the histogram from (1) to the fitted curve, and compute a goodness-of-fit value, such as $\\chi^2$/d.f.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import scipy.optimize as fitter\n",
    "\n",
    "\n",
    "# Once again, feel free to play around with the matplotlib parameters\n",
    "plt.rcParams['figure.figsize'] = 8,4\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "\n",
    "energies = np.loadtxt('peak.dat') # MeV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall `plt.hist()` isn't great when you need error bars, so it's better to first use [`np.histogram()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html) -- which returns the counts in each bin, along with the edges of the bins (there are $n + 1$ edges for $n$ bins).  Once you find the bin centers and errors on the counts, you can make the actual plot with [`plt.bar()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.bar.html).  Start with something close to `bins = 25` as the second input parameter to `np.histogram()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy.histogram to get the counts and bin edges\n",
    "\n",
    "# bin_centers = 0.5*(bin_edges[1:]+bin_edges[:-1]) works for finding the bin centers\n",
    "\n",
    "# assume Poisson errors on the counts – errors go as the square root of the count\n",
    "\n",
    "# now use plt.bar() to make the histogram with error bars (remember to label the plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the list of `energies` directly as input to `scipy.stats.norm.fit()`; the returned values are the mean and standard deviation of a fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean and standard deviation using scipy.stats.norm.fit()\n",
    "# Compare these to those computed in the previous homework (or just find them again here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the binned values (found above with `np.histogram()`) and their errors use `scipy.optimize.curve_fit()` to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, curve_fit() will need a model function defined\n",
    "def model(x, A, mu, sigma):\n",
    "    '''Model function to use with curve_fit();\n",
    "       it should take the form of a 1-D Gaussian'''\n",
    "\n",
    "# Also make sure you define some starting parameters for curve_fit (we typically called these par0 or p0 in the past workshop)\n",
    "\n",
    "'''# You can use this to ensure the errors are greater than 0 to avoid division by 0 within fitter.curve_fit()\n",
    "for i, err in enumerate(counts_err):\n",
    "    if err == 0:\n",
    "        counts_err[i] = 1'''\n",
    "\n",
    "# Now use fitter.curve_fit() on the binned data and compare the best-fit parameters to those found by scipy.stats.norm.fit()\n",
    "# It's also useful to plot the fitted curve over the histogram you made in part 1 to check that things are working properly\n",
    "\n",
    "# At this point, it's also useful to find the chi^2 and reduced chi^2 value of this binned fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this process with twice as many bins (i.e. now use `bins = 50` in `np.histogram()`, or a similar value).  Compute the $\\chi^2$ and reduced $\\chi^2$ and compare these values, along with the best-fit parameters between the two binned fits.  Feel free to continue to play with the number of bins and see how it changes the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
